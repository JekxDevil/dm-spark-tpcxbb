{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4adb557a-3b01-4e8b-be1d-dafc9b110432",
   "metadata": {},
   "source": [
    "# Apache Spark for complex queries\n",
    "\n",
    "## Data Management Homework 7\n",
    "\n",
    "In this assignment we will use \n",
    "[Apache Spark](https://spark.apache.org/): \n",
    "a popular framework for optimal distributed processing on large amount of data. \n",
    "The objective of is to use Apache Spark to translate and execute some queries of the TPCx-BB bigdata benchmark.  \n",
    "[TPCx-BB](https://www.tpc.org/tpc_documents_current_versions/current_specifications5.asp) \n",
    "or simply \"Big Bench\" is a common benchmark suite to evaluate the system performance on big data analytics and machine learning algorithms. \n",
    "We will focus on big data analytical queries, which are expressed in SQL. \n",
    "\n",
    "Spark is a framework available in multiple languages: Scala, Java, Python, R. In this exercise, we will use Python.\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Jupyter Lab\n",
    "\n",
    "If you are not familiar with the Jupyter Lab environment, check out these resources from the official website: \n",
    "[example notebook](https://jupyter.org/try), \n",
    "[docs](https://jupyterlab.readthedocs.io/en/stable/getting_started/overview.html). \n",
    "\n",
    "Quick reference:\n",
    "- This is a cell. A cell can contain either Markdown text (such as this one) or code. Everything in jupyter notebook is a cell.\n",
    "    - Click on the plus on the top bar to add a new cell\n",
    "    - You can double-click on a text cell to edit iy using Markdown\n",
    "    - You can run a cell by either using the button \"play\" at the top bar or by using the \"shift + enter\" key combination\n",
    "    - Running a code cell executes it\n",
    "    - Running a text cell formats the text\n",
    "- Once you run a cell it stays in memory! So code will be run based on which order you execute cells, even if you execute a cell that is below another one before\n",
    "- General rule #1: try to arrange cell step-by-stop from top to bottom. If anything breaks, try to execute every cell from the top\n",
    "- General rule #2: if you are stuck or a cell is blocked during execution re-run the kernel from the top bar menu\n",
    " \n",
    "### Contents\n",
    "You can navigate through this exercise contents with the file explorer on the left.  \n",
    "The contents are \"extracted\" from the \n",
    "[TPCx-BB](https://www.tpc.org/tpc_documents_current_versions/current_specifications5.asp) \n",
    "benchmark source folder. \n",
    "Please refer to the link if you want to have a broader overview and/or additional information TPCx-BB. \n",
    "Since this exercise differs from the actual benchmark, only a subset of its content are reported here:\n",
    "- `queries/` contains 30 SQL/Spark queries, some of which are to be ported to Spark in this exercise. Every query `qxx/` folder (`xx` = number) contains\n",
    "    - `engineLocalSettings.conf`: TPC related, disregard\n",
    "    - `engineLocalSettings.sql`: TPC related, disregard\n",
    "    - `explain_qxx.sql`: *query content* in \"explanatory\" format\n",
    "    - `qxx.sql`: *query content* in TPC exec format\n",
    "    - `run.sh`: TPC related, disregard\n",
    "    - `results/qxx-result`: contains the expect result in plain-text. You should compare this with your query output (example provided later)\n",
    "- `spark_table_schemas`: contains schema information for every table in the dataset. Not relevant for the implementation\n",
    "- `TPCx-BB-dataset`: contains all the tables in separate folder. Refer to it for table names\n",
    "\n",
    "**Do not modify** `spark_table_schemas` or `TPCx-BB-dataset` contents as it may compromise your solution.\n",
    "\n",
    "### Guidelines\n",
    "You must use the Spark SQL module to solve this exercise. Refer to the official documentation:\n",
    "> Spark SQL: https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "\n",
    "We will work with *DataFrames*: a Spark data type used to represent collections of data, including database Tables. \n",
    "You are strongly recommended to refer to the DataFrame API reference within the Spark SQL module during the exercise implementation. \n",
    "There you will find methods, functions and further datatypes which are equivalent to SQL operations. \n",
    "> DataFrame Reference: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html\n",
    "The Spark DataFrame API resembles the one of Pandas library.\n",
    " \n",
    "Reference: \n",
    " [PySpark API documentation](https://spark.apache.org/docs/latest/api/python/reference/index.html)\n",
    "\n",
    "#### Reading TCPx-BB queries\n",
    "The SQL queries files (`explain_qxx.sql` and `qxx.sql`) are taken directly from the TPCx-BB benchmark suite \n",
    "and therefore might contain \"extra\" SQL statements and comments, \n",
    "which are functional to the TCPx-BB original benchmark (e.g. `hive` instructions, `EXPLAIN`, etc.). \n",
    "Your goal is to extract and translate the SQL query only, disregarding irrelevant statements/instructions for the purpose of this exercise.  \n",
    "Additionally, queries might contain *template* variables, in the form `${qxx_variable_name}`. \n",
    "You can find all relative templates in the `query/queryParameters.sql` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0ec252-f548-457d-b2f8-46f635adcb68",
   "metadata": {},
   "source": [
    "## Environment preparation\n",
    "**Make sure to read through and run the following code cells before starting the exercise** \n",
    "\n",
    "### Install PySpark\n",
    "Run the cell below to install Spark for Python. If it does not work, close and restart Jupyter or execute the command below on a separate terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9463c8c5-6109-4387-8d88-c7d9ee5a1554",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T16:41:46.093817Z",
     "start_time": "2024-04-16T16:41:45.081651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /home/gief/.cache/pypoetry/virtualenvs/assignment-07-spark-Odt_KzyD-py3.10/lib/python3.10/site-packages (3.5.1)\r\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/gief/.cache/pypoetry/virtualenvs/assignment-07-spark-Odt_KzyD-py3.10/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd11ca10-844d-4fad-b9ea-180d34bc0d69",
   "metadata": {},
   "source": [
    "### Import PySpark\n",
    "\n",
    "Whenever working with Spark, you need to either start a Spark Session.\n",
    "The Spark Session Builder will handle under-the-hood the architecture of the framework discussed in class \n",
    "and give us an entry point to programming with Spark .\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "id": "b8f581f6-d3e1-43ef-aab5-47fdfcfc246f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T22:05:39.883385Z",
     "start_time": "2024-04-16T22:05:27.822607Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import os\n",
    "\n",
    "# when run locally, spark has one (master) node\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"Homework 07\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/17 00:05:32 WARN Utils: Your hostname, gief-pc resolves to a loopback address: 127.0.1.1; using 192.168.1.25 instead (on interface wlp0s20f3)\n",
      "24/04/17 00:05:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/17 00:05:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x70c1a868b6d0>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.25:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Homework 07</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "87f6aff9-3027-4c1f-af23-de85a737a5c9",
   "metadata": {},
   "source": [
    "### Define Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "id": "d555d417-51bb-4d91-a7a0-5013581ef199",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-04-16T23:25:58.249977Z",
     "start_time": "2024-04-16T23:25:58.235362Z"
    }
   },
   "source": [
    "def get_table(name):\n",
    "    \"\"\"\n",
    "    Load a table from the TPCx-BB dataset.\n",
    "    The dataframe is read from the Parquet format, \n",
    "    which is a columnar storage format ideal for efficient storage and retrieval of data\n",
    "    :param name: name of the table \n",
    "    :return: a Spark DataFrame of the table\n",
    "    \"\"\"\n",
    "    df = spark.read.parquet(f\"TPCx-BB-dataset/{name}.ptxt\")\n",
    "    f = open(f\"spark_table_schemas/{name}.schema\",\"r\")\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        l = line.split()\n",
    "        if len(l) > 2:\n",
    "            df.schema[l[0]].nullable = False\n",
    "\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "id": "2b2cc9a0-c568-4a2d-822f-f9e60b67d819",
   "metadata": {},
   "source": [
    "### Explore the dataset\n",
    "You can use `get_table` to load current dataset tables. A table in Spark is stored as a *DataFrame* - see reference in the exercise intro."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# load the current table \n",
    "customer = get_table(\"customer\")\n",
    "customer.show() # show the first 20 rows"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T23:26:21.884110Z",
     "start_time": "2024-04-16T23:26:21.069012Z"
    }
   },
   "id": "2a1950967067b488",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+------------------+------------------+-----------------+----------------------+---------------------+------------+------------+-----------+---------------------+-----------+-------------+------------+--------------------+-------------+--------------------+------------------+\n",
      "|c_customer_sk|   c_customer_id|c_current_cdemo_sk|c_current_hdemo_sk|c_current_addr_sk|c_first_shipto_date_sk|c_first_sales_date_sk|c_salutation|c_first_name|c_last_name|c_preferred_cust_flag|c_birth_day|c_birth_month|c_birth_year|     c_birth_country|      c_login|     c_email_address|c_last_review_date|\n",
      "+-------------+----------------+------------------+------------------+-----------------+----------------------+---------------------+------------+------------+-----------+---------------------+-----------+-------------+------------+--------------------+-------------+--------------------+------------------+\n",
      "|            0|AAAAAAAAAAAAAAAA|           1824793|              3203|             2555|                 28776|                14690|         Ms.|      Marisa| Harrington|                    N|         17|            4|        1988|UNITED ARAB EMIRATES| RRCyuY3XfE3a|Marisa.Harrington...|          gdMmGdU9|\n",
      "|            1|AAAAAAAAAAAAAAAB|            830976|              2600|             9191|                 94658|                19931|        Miss|      Bessie|   Calderon|                    N|         20|           10|        1945|              MONACO|      aCJb9cc|Bessie.Calderon@s...|         hLQn0LtVa|\n",
      "|            2|AAAAAAAAAAAAAAAC|            335540|              5527|            14091|                 31783|               106195|        Miss|     Barbara|     Hoover|                    Y|          3|            4|        1952|           GREENLAND|   59cfL9Fbpv|Barbara.Hoover@la...|          vDVbSOCI|\n",
      "|            3|AAAAAAAAAAAAAAAD|           1056662|              1978|            19122|                  8741|                87583|         Mr.|       Lanny|       Berg|                    N|          2|            1|        1932|              CANADA|    TvsKkZgPt|   Lanny.Berg@gmx.fr|          ZxCh4LKH|\n",
      "|            4|AAAAAAAAAAAAAAAE|            596707|              3971|            21996|                 14688|                75952|        Mrs.|        Gina|      Felix|                    Y|         18|            4|        1959|               NAURU|     t3r2t4rC|Gina.Felix@amplim...|            WpJjJ1|\n",
      "|            5|AAAAAAAAAAAAAAAF|            363428|              4888|            21272|                 38182|                65826|         Dr.|      Judith|     Mccain|                    N|         11|           10|        1936|             ICELAND|          WfT|Judith.Mccain@pri...|          jXmMzRt8|\n",
      "|            6|AAAAAAAAAAAAAAAG|           1601629|              6779|            18776|                 48020|                75950|         Sir|      Lavern|     Guzman|                    N|          2|            7|        1952|                GUAM|     SD7IbZaQ|Lavern.Guzman@pea...|            xKpEnQ|\n",
      "|            7|AAAAAAAAAAAAAAAH|            260357|              2400|            30589|                 20361|                42197|         Dr.|      Janice|     Murray|                    Y|         23|            8|        1976|           LITHUANIA|   p1FXddH8rW|Janice.Murray@mac...|         xhjGNk98a|\n",
      "|            8|AAAAAAAAAAAAAAAI|           1519111|              1818|            23037|                 39194|                62551|         Mr.|         Lee|    Jenkins|                    N|         15|           12|        1972|             TUNISIA|         s2Ef|Lee.Jenkins@hushm...|                 j|\n",
      "|            9|AAAAAAAAAAAAAAAJ|            555297|              5958|            30815|                 43531|                50147|         Mr.|    Jonathan|     Parker|                    Y|         10|            2|        1976|             GEORGIA|       N2Qq55|Jonathan.Parker@m...|            vgudeV|\n",
      "|           10|AAAAAAAAAAAAAAAK|            890454|              5535|            34947|                 78774|               102861|         Sir|      George|   Anderson|                    N|         14|            3|        1968|             TUNISIA|           Sa|George.Anderson@s...|                46|\n",
      "|           11|AAAAAAAAAAAAAAAL|           1438986|              1495|              761|                 91923|                70592|         Sir|       Jorge|     Taylor|                    N|         22|            2|        1963|               NIGER|zIhZa6uiGHzb6| Jorge.Taylor@ip6.li|        bmZgmTmXHY|\n",
      "|           12|AAAAAAAAAAAAAAAM|           1425262|              1525|            44513|                 10286|               103343|         Dr.|      Cheryl|        Lee|                    Y|         12|           11|        1972|             JAMAICA|     tBf8lG0t|Cheryl.Lee@lavabi...|             h3tgo|\n",
      "|           13|AAAAAAAAAAAAAAAN|           1331330|              2300|            13401|                 51231|               102876|         Mr.|         Don|   Williams|                    Y|         15|            7|        1952|              BRAZIL|     cb00ffNo|Don.Williams@gmx....|        aEV7ed7nMQ|\n",
      "|           14|AAAAAAAAAAAAAAAO|           1633793|              5097|            21014|                 71728|                 1803|         Ms.|      Alicia|     Flores|                    Y|         15|           10|        1983|         AFGHANISTAN|          Y3e|Alicia.Flores@gmx.hk|         rsOpe2e5s|\n",
      "|           15|AAAAAAAAAAAAAAAP|            686483|              2725|            25060|                 44067|                61458|         Sir|        Paul|    Becerra|                    N|         13|            2|        1955|        COOK ISLANDS|  eILOny1Kpzq|Paul.Becerra@gmai...|          EGo3wwNs|\n",
      "|           16|AAAAAAAAAAAAAAAQ|            885432|               808|            42429|                 58839|                53580|         Sir|       Ellis| Cunningham|                    Y|         19|            6|        1981|                CUBA|            L|Ellis.Cunningham@...|                hw|\n",
      "|           17|AAAAAAAAAAAAAAAR|           1536897|              1789|            10433|                 34329|                88531|         Mr.|       Lance|     Hardee|                    Y|          2|            2|        1982|SYRIAN ARAB REPUBLIC|          1iF| Lance.Hardee@ip6.li|            XPcaEU|\n",
      "|           18|AAAAAAAAAAAAAAAS|           1025955|              7158|            47874|                 93009|                88336|        Miss|      Alisha|    Jenkins|                    N|          4|           11|        1927|         ISLE OF MAN|  VTZFhF0rDxe|Alisha.Jenkins@we...|          czjma9gS|\n",
      "|           19|AAAAAAAAAAAAAAAT|           1384075|              3101|            32881|                 15482|                52298|        Mrs.|        Adam|    Quigley|                    N|         14|            7|        1966|               CHINA|  LXkGj0iRk5X|Adam.Quigley@vfem...|         IwvFu8IYR|\n",
      "+-------------+----------------+------------------+------------------+-----------------+----------------------+---------------------+------------+------------+-----------+---------------------+-----------+-------------+------------+--------------------+-------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T23:39:37.777233Z",
     "start_time": "2024-04-16T23:39:35.873430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Creating a DataFrame from scratch\n",
    "df = spark.createDataFrame(data=[\n",
    "    (1, \"Alice\", 34),\n",
    "    (2, \"Bob\", 45),\n",
    "    (3, \"Charlie\", 56)\n",
    "], schema=[\"id\", \"name\", \"age\"])\n",
    "\n",
    "# some useful methods to explore the table in dataframe\n",
    "print(f'Type: {type(df) = }',    # PySpark Dataframe is not equal to Pandas Dataframe !\n",
    "      f'First top n=2 rows: {df.head(2) = }',\n",
    "      f'Column title names: {df.columns = }',\n",
    "      f'Column title names with its types: {df.dtypes = }',\n",
    "      f'Selecting a column: {df.select(\"name\") = }', # it's still a DataFrame\n",
    "      f'Avoid pandas notation (less features): {df[\"name\"] = }', # Column object with lesser features than PySpark DataFrame\n",
    "      f'Selecting more columns: {df.select([\"name\", \"age\"]) = }', # it's still a DataFrame\n",
    "      sep='\\n\\n')\n",
    "\n",
    "# print the values of the selected columns\n",
    "df.select([\"name\", \"age\"]).show()\n",
    "\n",
    "# get statistics retrieved from a dataframe, computationally expensive, similar to Pandas\n",
    "df.describe().show()"
   ],
   "id": "9582f10568cf2464",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: type(df) = <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "\n",
      "First top n=2 rows: df.head(2) = [Row(id=1, name='Alice', age=34), Row(id=2, name='Bob', age=45)]\n",
      "\n",
      "Column title names: df.columns = ['id', 'name', 'age']\n",
      "\n",
      "Column title names with its types: df.dtypes = [('id', 'bigint'), ('name', 'string'), ('age', 'bigint')]\n",
      "\n",
      "Selecting a column: df.select(\"name\") = DataFrame[name: string]\n",
      "\n",
      "Avoid pandas notation (less features): df[\"name\"] = Column<'name'>\n",
      "\n",
      "Selecting more columns: df.select([\"name\", \"age\"]) = DataFrame[name: string, age: bigint]\n",
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|  Alice| 34|\n",
      "|    Bob| 45|\n",
      "|Charlie| 56|\n",
      "+-------+---+\n",
      "\n",
      "+-------+---+-------+----+\n",
      "|summary| id|   name| age|\n",
      "+-------+---+-------+----+\n",
      "|  count|  3|      3|   3|\n",
      "|   mean|2.0|   NULL|45.0|\n",
      "| stddev|1.0|   NULL|11.0|\n",
      "|    min|  1|  Alice|  34|\n",
      "|    max|  3|Charlie|  56|\n",
      "+-------+---+-------+----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "id": "251c2dd5-4548-4e7b-9fec-30c559590acb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-04-16T23:11:26.871153Z",
     "start_time": "2024-04-16T23:11:26.610660Z"
    }
   },
   "source": [
    "df = spark.read.option('header', 'true').parquet('TPCx-BB-dataset/customer.ptxt')\n",
    "df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[c_customer_sk: bigint, c_customer_id: string, c_current_cdemo_sk: bigint, c_current_hdemo_sk: bigint, c_current_addr_sk: bigint, c_first_shipto_date_sk: bigint, c_first_sales_date_sk: bigint, c_salutation: string, c_first_name: string, c_last_name: string, c_preferred_cust_flag: string, c_birth_day: bigint, c_birth_month: bigint, c_birth_year: bigint, c_birth_country: string, c_login: string, c_email_address: string, c_last_review_date: string]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Column operations on DataFrame\n",
    "\n",
    "The columns operations generate a new DataFrame that need to be stored in a variable to be saved.\n",
    "It does not change the original DataFrame, but creates a new modified one from it."
   ],
   "id": "658ade0815720afb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T23:42:32.811967Z",
     "start_time": "2024-04-16T23:42:32.252247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# add a column\n",
    "df_added_column = df.withColumn(colName=\"age in 2 years\", col=col(\"age\") + 2)\n",
    "df_added_column.show()"
   ],
   "id": "ea339233ec757bee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+--------------+\n",
      "| id|   name|age|age in 2 years|\n",
      "+---+-------+---+--------------+\n",
      "|  1|  Alice| 34|            36|\n",
      "|  2|    Bob| 45|            47|\n",
      "|  3|Charlie| 56|            58|\n",
      "+---+-------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T23:43:12.516555Z",
     "start_time": "2024-04-16T23:43:11.787730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# remove a column\n",
    "df_removed_column = df_added_column.drop(\"id\")\n",
    "df_removed_column.show()"
   ],
   "id": "b25db971e45320b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------------+\n",
      "|   name|age|age in 2 years|\n",
      "+-------+---+--------------+\n",
      "|  Alice| 34|            36|\n",
      "|    Bob| 45|            47|\n",
      "|Charlie| 56|            58|\n",
      "+-------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T23:49:45.761467Z",
     "start_time": "2024-04-16T23:49:45.285827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# rename a column\n",
    "df_renamed_column = df_removed_column.withColumnRenamed(existing=\"name\", new=\"name_column_renamed\")\n",
    "df_renamed_column.show()"
   ],
   "id": "1255306cbfe88d70",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+--------------+\n",
      "|name_column_renamed|age|age in 2 years|\n",
      "+-------------------+---+--------------+\n",
      "|              Alice| 34|            36|\n",
      "|                Bob| 45|            47|\n",
      "|            Charlie| 56|            58|\n",
      "+-------------------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "id": "4c673e93-8be4-493b-974c-dc75aa7533c7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-04-16T19:55:37.335276Z",
     "start_time": "2024-04-16T19:55:37.214780Z"
    }
   },
   "source": [
    "# show the 1st row of the customer table\n",
    "customer.show(1) "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+------------------+------------------+-----------------+----------------------+---------------------+------------+------------+-----------+---------------------+-----------+-------------+------------+--------------------+------------+--------------------+------------------+\n",
      "|c_customer_sk|   c_customer_id|c_current_cdemo_sk|c_current_hdemo_sk|c_current_addr_sk|c_first_shipto_date_sk|c_first_sales_date_sk|c_salutation|c_first_name|c_last_name|c_preferred_cust_flag|c_birth_day|c_birth_month|c_birth_year|     c_birth_country|     c_login|     c_email_address|c_last_review_date|\n",
      "+-------------+----------------+------------------+------------------+-----------------+----------------------+---------------------+------------+------------+-----------+---------------------+-----------+-------------+------------+--------------------+------------+--------------------+------------------+\n",
      "|            0|AAAAAAAAAAAAAAAA|           1824793|              3203|             2555|                 28776|                14690|         Ms.|      Marisa| Harrington|                    N|         17|            4|        1988|UNITED ARAB EMIRATES|RRCyuY3XfE3a|Marisa.Harrington...|          gdMmGdU9|\n",
      "+-------------+----------------+------------------+------------------+-----------------+----------------------+---------------------+------------+------------+-----------+---------------------+-----------+-------------+------------+--------------------+------------+--------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "281d2927-53f1-4c79-9880-fe7443096385",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-04-16T23:44:11.870352Z",
     "start_time": "2024-04-16T23:44:11.856763Z"
    }
   },
   "source": [
    "# display the table schema, which is Spark is a set of [column, type, nullable]\n",
    "customer.schema "
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('c_customer_sk', LongType(), False), StructField('c_customer_id', StringType(), False), StructField('c_current_cdemo_sk', LongType(), True), StructField('c_current_hdemo_sk', LongType(), True), StructField('c_current_addr_sk', LongType(), True), StructField('c_first_shipto_date_sk', LongType(), True), StructField('c_first_sales_date_sk', LongType(), True), StructField('c_salutation', StringType(), True), StructField('c_first_name', StringType(), True), StructField('c_last_name', StringType(), True), StructField('c_preferred_cust_flag', StringType(), True), StructField('c_birth_day', LongType(), True), StructField('c_birth_month', LongType(), True), StructField('c_birth_year', LongType(), True), StructField('c_birth_country', StringType(), True), StructField('c_login', StringType(), True), StructField('c_email_address', StringType(), True), StructField('c_last_review_date', StringType(), True)])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T23:44:15.325756Z",
     "start_time": "2024-04-16T23:44:15.311414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# or for a nice pretty print tree view of it\n",
    "customer.printSchema()"
   ],
   "id": "537c963f86dd70ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- c_customer_sk: long (nullable = true)\n",
      " |-- c_customer_id: string (nullable = true)\n",
      " |-- c_current_cdemo_sk: long (nullable = true)\n",
      " |-- c_current_hdemo_sk: long (nullable = true)\n",
      " |-- c_current_addr_sk: long (nullable = true)\n",
      " |-- c_first_shipto_date_sk: long (nullable = true)\n",
      " |-- c_first_sales_date_sk: long (nullable = true)\n",
      " |-- c_salutation: string (nullable = true)\n",
      " |-- c_first_name: string (nullable = true)\n",
      " |-- c_last_name: string (nullable = true)\n",
      " |-- c_preferred_cust_flag: string (nullable = true)\n",
      " |-- c_birth_day: long (nullable = true)\n",
      " |-- c_birth_month: long (nullable = true)\n",
      " |-- c_birth_year: long (nullable = true)\n",
      " |-- c_birth_country: string (nullable = true)\n",
      " |-- c_login: string (nullable = true)\n",
      " |-- c_email_address: string (nullable = true)\n",
      " |-- c_last_review_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "id": "cf42daac-7ec5-4a12-b02d-c228dcef9940",
   "metadata": {},
   "source": [
    "## Sample query translation\n",
    "Refer to `queries/q00/explain_q00.sql`. The code below is a valid translation of that query using SparkSQL. You can use any methods in the Spark SQL DataFrame class to implement your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b893fb04-c08c-4773-962f-413d08788d34",
   "metadata": {},
   "source": [
    "### Query 0\n",
    "Find the amount of items sold by their category.  \n",
    "Only in certain categories sold in specific stores are considered,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c0de66c-e2bb-4369-a2ed-efbe483764d4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-04-16T16:43:07.225831Z",
     "start_time": "2024-04-16T16:43:05.596689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|    i_category|count|\n",
      "+--------------+-----+\n",
      "|Home & Kitchen| 1975|\n",
      "|         Music|25060|\n",
      "+--------------+-----+\n"
     ]
    }
   ],
   "source": [
    "## query0\n",
    "\n",
    "## look in TCPx-BB-dataset to check all the available tables.\n",
    "s = get_table(\"store_sales\")\n",
    "i = get_table(\"item\")\n",
    "\n",
    "query0_solution = s.join(i, s.ss_item_sk == i.i_item_sk) \\\n",
    "                .filter(i.i_category_id < 3) \\\n",
    "                .filter(s.ss_store_sk.isin([10, 20, 33, 40, 50])) \\\n",
    "                .groupBy(\"i_category\") \\\n",
    "                .count()\n",
    "\n",
    "query0_solution.show()      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57618142-e6a2-4578-8b01-99cd4f6fffae",
   "metadata": {},
   "source": [
    "The cell below is a shortcut to display the results file of q00 without navigating to the file.  \n",
    "The `!` symbol followed by a bash command (`cat` in this case) can be used as in-cell access to the terminal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "028cbc53-1875-41e2-a94e-f1fd9f81022d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home & Kitchen, 1975\n",
      "Music, 25060"
     ]
    }
   ],
   "source": [
    "## check the result\n",
    "!cat queries/q00/results/q00-result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7864b593-0936-453d-8e47-0ac2300288dd",
   "metadata": {},
   "source": [
    "## [YOUR SOLUTION BELOW]\n",
    "Write the query description in a Markdown cell, followed by a code cell with the query implementation.  \n",
    "Query descriptions can be found in the TCPx-BB specification, page 93: https://www.tpc.org/tpc_documents_current_versions/current_specifications5.asp\n",
    "\n",
    "You should implement all the queries assigned in the homework sheet, plus the optional ones if you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45aecb8-e4dd-4676-a469-0a82ee34bbfe",
   "metadata": {},
   "source": [
    "### Query X\n",
    "description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6d3e2faf-51c6-4484-b66c-1abe42049b8e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "# implementation\n",
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bb26ea2-e6f1-4a41-8d9d-40a76a80d3a6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09c3dcb-6122-4ee3-a483-900b118da67b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
